{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a328c3a4",
   "metadata": {},
   "source": [
    "# Policy Gradient-Based Deterministic Optimal Savings\n",
    "\n",
    "Author: [John Stachurski](https://johnstachurski.net)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we solve a deterministic infinite horizon optimal savings\n",
    "problem using policy gradient ascent with JAX. \n",
    "\n",
    "Each policy is represented as a fully connected feed forward neural network.\n",
    "\n",
    "Utility takes the CRRA form $u(c) = c^{1-\\gamma} / (1-\\gamma)$ and the discount factor is $\\beta$.\n",
    "\n",
    "Wealth evolves according to \n",
    "\n",
    "$$\n",
    "    w' = R (w - c) \n",
    "$$\n",
    "\n",
    "where $R > 0$ is the gross interest rate.  \n",
    "\n",
    "To ensure stability we check that $\\beta R^{1-\\gamma} < 1$.\n",
    "\n",
    "For this model, it is known that the optimal policy is $c = \\kappa w$, where\n",
    "\n",
    "$$\n",
    "    \\kappa := 1 - [\\beta R^{1-\\gamma}]^{1/\\gamma}\n",
    "$$\n",
    "\n",
    "We use this known exact solution to check our numerical methods.\n",
    "\n",
    "Initial wealth $w_0$ is fixed at 1.0, so the objective function is\n",
    "\n",
    "$$\n",
    "    \\max_{\\sigma \\in \\Sigma} v_\\sigma(w_0)\n",
    "    \\quad \\text{with} \\quad w_0 := 1.0\n",
    "$$\n",
    "\n",
    "Here \n",
    "\n",
    "* $\\Sigma$ is the set of all feasible policies and\n",
    "* $v_\\sigma(w)$ is the lifetime value of following stationary policy $\\sigma$, given initial wealth $w$.\n",
    "\n",
    "We begin with some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8bcd1",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "We use a class called `Model` to store model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(NamedTuple):\n",
    "    \"\"\"\n",
    "    Stores parameters for the model.\n",
    "\n",
    "    \"\"\"\n",
    "    γ: float = 0.2\n",
    "    β: float = 0.96\n",
    "    R: float = 1.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac159902",
   "metadata": {},
   "source": [
    "We use a class called `LayerParams` to store parameters representing a single\n",
    "layer of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb270ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerParams(NamedTuple):\n",
    "    \"\"\"\n",
    "    Stores parameters for one layer of the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    W: jnp.ndarray     # weights\n",
    "    b: jnp.ndarray     # biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b2a0f",
   "metadata": {},
   "source": [
    "The next class stores some fixed values that form part of the network training\n",
    "configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration and parameters for training the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    seed = 42                    # Seed for network initialization\n",
    "    epochs = 400                 # No of training epochs\n",
    "    path_length = 500            # Length of each consumption path\n",
    "    layer_sizes = 1, 32, 32, 1   # Network layer sizes\n",
    "    init_lr = 0.001              # Learning rate schedule parameter\n",
    "    min_lr = 0.000005            # Learning rate schedule parameter\n",
    "    warmup_steps = 200           # Learning rate schedule parameter\n",
    "    decay_steps = 700            # Learning rate schedule parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8b945",
   "metadata": {},
   "source": [
    "The following function initializes a single layer of the network using Le Cun\n",
    "initialization.\n",
    "\n",
    "(Le Cun initialization is thought to pair well with `selu` activation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1405b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_layer(in_dim, out_dim, key):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for a single layer of a the network.\n",
    "    Use LeCun initialization.\n",
    "\n",
    "    \"\"\"\n",
    "    s = jnp.sqrt(1.0 / in_dim)\n",
    "    W = jax.random.normal(key, (in_dim, out_dim)) * s\n",
    "    b = jnp.zeros((out_dim,))\n",
    "    return LayerParams(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5cac1",
   "metadata": {},
   "source": [
    "The next function builds an entire network, as represented by its parameters, by\n",
    "initializing layers and stacking them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7fc001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(key, layer_sizes):\n",
    "    \"\"\"\n",
    "    Build a network by initializing all of the parameters.\n",
    "    A network is a list of LayerParams instances, each \n",
    "    containing a weight-bias pair (W, b).\n",
    "\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    # For all layers but the output layer\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        # Build the layer \n",
    "        key, subkey = jax.random.split(key)\n",
    "        layer = initialize_layer(\n",
    "            layer_sizes[i],      # in dimension for layer\n",
    "            layer_sizes[i + 1],  # out dimension for layer\n",
    "            subkey \n",
    "        )\n",
    "        # And add it to the parameter list\n",
    "        params.append(layer)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc7eee",
   "metadata": {},
   "source": [
    "Now we provide a function to do a forward pass through the network, given the\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c197e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(params, w):\n",
    "    \"\"\"\n",
    "    Evaluate neural network policy: maps a given wealth level w to\n",
    "    consumption rate c/w by running a forward pass through the network.\n",
    "\n",
    "    \"\"\"\n",
    "    σ = jax.nn.selu          # Activation function\n",
    "    x = jnp.array((w,))      # Make state a 1D array\n",
    "    # Forward pass through network, without the last step\n",
    "    for W, b in params[:-1]:\n",
    "        x = σ(x @ W + b)\n",
    "    # Complete with sigmoid activation for consumption rate\n",
    "    # Direct output in [0, 0.99] range\n",
    "    W, b = params[-1]\n",
    "    x = jax.nn.sigmoid(x @ W + b) * 0.99  # Direct output in [0, 0.99]\n",
    "    # Extract and return consumption rate\n",
    "    consumption_rate = x[0]\n",
    "    return consumption_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86273c2",
   "metadata": {},
   "source": [
    "We use CRRA utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a909544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def u(c, γ):\n",
    "    \"\"\" Utility function. \"\"\"\n",
    "    c = jnp.maximum(c, 1e-10)\n",
    "    return c**(1 - γ) / (1 - γ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3128e8f7",
   "metadata": {},
   "source": [
    "The next function approximates lifetime value associated with a given policy, as\n",
    "represented by the parameters of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=('path_length'))\n",
    "def compute_lifetime_value(params, model, path_length):\n",
    "    \"\"\"\n",
    "    Compute the lifetime value of a path generated from\n",
    "    the policy embedded in params and the initial condition w_0 = 1.\n",
    "\n",
    "    \"\"\"\n",
    "    γ, β, R = model.γ, model.β, model.R\n",
    "    initial_w = 1.0\n",
    "\n",
    "    def update(t, state):\n",
    "        # Unpack and compute consumption given current wealth\n",
    "        w, value, discount = state\n",
    "        consumption_rate = forward(params, w)\n",
    "        c = consumption_rate * w\n",
    "        # Update loop state and return it\n",
    "        w = R * (w - c)\n",
    "        value = value + discount * u(c, γ)\n",
    "        discount = discount * β\n",
    "        new_state = w, value, discount\n",
    "        return new_state\n",
    "\n",
    "    initial_value, initial_discount = 0.0, 1.0\n",
    "    initial_state = initial_w, initial_value, initial_discount\n",
    "    final_w, final_value, discount = jax.lax.fori_loop(\n",
    "        0, path_length, update, initial_state\n",
    "    )\n",
    "    return final_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9f75c",
   "metadata": {},
   "source": [
    "Here's the loss function we will minimize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975dc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(params, model, path_length):\n",
    "    \"\"\"\n",
    "    Loss is the negation of the lifetime value of the policy \n",
    "    identified by `params`.\n",
    "\n",
    "    \"\"\"\n",
    "    return -compute_lifetime_value(params, model, path_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aee061",
   "metadata": {},
   "source": [
    "We create a standard Optax learning rate scheduler, which controls the time path\n",
    "of the learning parameter over the process of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lr_schedule():\n",
    "    warmup_fn = optax.linear_schedule(\n",
    "        init_value=0.0,\n",
    "        end_value=Config.init_lr,\n",
    "        transition_steps=Config.warmup_steps\n",
    "    )\n",
    "    \n",
    "    decay_fn = optax.exponential_decay(\n",
    "        init_value=Config.init_lr,\n",
    "        transition_steps=Config.decay_steps,\n",
    "        decay_rate=0.5,\n",
    "        end_value=Config.min_lr\n",
    "    )\n",
    "    \n",
    "    return optax.join_schedules(\n",
    "        schedules=[warmup_fn, decay_fn],\n",
    "        boundaries=[Config.warmup_steps]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d73c5e",
   "metadata": {},
   "source": [
    "## Train and solve \n",
    "\n",
    "First we create an instance of the model and unpack names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d777923",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "γ, β, R = model.γ, model.β, model.R\n",
    "seed, epochs = Config.seed, Config.epochs\n",
    "path_length = Config.path_length\n",
    "layer_sizes = Config.layer_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec050536",
   "metadata": {},
   "source": [
    "We test stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert β * R**(1 - γ) < 1, \"Parameters fail stability test.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63f508",
   "metadata": {},
   "source": [
    "We compute the optimal consumption rate and lifetime value from the analytical\n",
    "expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c09491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "κ = 1 - (β * R**(1 - γ))**(1/γ)\n",
    "print(f\"Optimal consumption rate = {κ}.\\n\")\n",
    "v_max = κ**(-γ) * u(1.0, γ)\n",
    "print(f\"Theoretical maximum lifetime value = {v_max}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c760f5f",
   "metadata": {},
   "source": [
    "Let's now create a learning rate schedule and set up the Optax minimizer, using\n",
    "[Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f30e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = create_lr_schedule()\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Gradient clipping for stability\n",
    "    optax.adam(learning_rate=lr_schedule)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe815cc",
   "metadata": {},
   "source": [
    "We initialize the parameters in the neural network and the state of the\n",
    "optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a17b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(seed)\n",
    "params = initialize_network(key, layer_sizes)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8965ed",
   "metadata": {},
   "source": [
    "Now let's train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_history = []\n",
    "best_value = -jnp.inf\n",
    "best_params = params\n",
    "for i in range(epochs):\n",
    "\n",
    "    # Compute value and gradients at existing parameterization\n",
    "    loss, grads = jax.value_and_grad(loss_function)(params, model, path_length)\n",
    "    lifetime_value = - loss\n",
    "    value_history.append(lifetime_value)\n",
    "\n",
    "    # Track best parameters\n",
    "    if lifetime_value > best_value:\n",
    "        best_value = lifetime_value\n",
    "        best_params = params\n",
    "\n",
    "    # Update parameters using optimizer\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}: Value = {lifetime_value:.4f}\")\n",
    "\n",
    "# Use best parameters instead of final\n",
    "params = best_params\n",
    "print(f\"\\nBest value: {best_value:.4f}\")\n",
    "print(f\"Final value: {value_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be820ed0",
   "metadata": {},
   "source": [
    "First we plot the evolution of lifetime value over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9753b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning progress\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(value_history, 'b-', linewidth=2)\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('policy value')\n",
    "ax.set_title('learning progress')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6111a9",
   "metadata": {},
   "source": [
    "Next we compare the learned and optimal policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_grid = jnp.linspace(0.01, 1.0, 1000)\n",
    "policy_vmap = jax.vmap(lambda w: forward(params, w))\n",
    "consumption_rate = policy_vmap(w_grid)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_grid, consumption_rate, linestyle='--', lw=4, label='learned policy')\n",
    "ax.plot(w_grid, κ * jnp.ones(len(w_grid)), lw=2, label='optimal')\n",
    "ax.set_xlabel('wealth')\n",
    "ax.set_ylabel('consumption rate (c/w)')\n",
    "ax.set_title('Consumption rate')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8a237",
   "metadata": {},
   "source": [
    "Let's have a look at paths for consumption and wealth under the learned and\n",
    "optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a98631",
   "metadata": {},
   "source": [
    "The figures below show that the learned policies are close to optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_consumption_path(params, T=120):\n",
    "    \"\"\"\n",
    "    Compute consumption path using neural network policy identified by params.\n",
    "\n",
    "    \"\"\"\n",
    "    w_sim = [1.0]   # 1.0 is the initial wealth\n",
    "    c_sim = []\n",
    "    w_opt = [1.0]\n",
    "    c_opt = []\n",
    "\n",
    "    w = 1.0\n",
    "    for t in range(T):\n",
    "\n",
    "        # Update policy path - forward returns consumption rate\n",
    "        c = forward(params, w) * w\n",
    "        c_sim.append(float(c))\n",
    "        w = R * (w - c)\n",
    "        w_sim.append(float(w))\n",
    "\n",
    "        if w <= 1e-10:\n",
    "            break\n",
    "\n",
    "    w = 1.0\n",
    "    for t in range(T):\n",
    "\n",
    "        # Update optimal path\n",
    "        c = κ * w\n",
    "        c_opt.append(c)\n",
    "        w = R * (w - c)\n",
    "        w_opt.append(w)\n",
    "\n",
    "        if w <= 1e-10:\n",
    "            break\n",
    "\n",
    "    return w_sim, c_sim, w_opt, c_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ee5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and plot path\n",
    "w_sim, c_sim, w_opt, c_opt = simulate_consumption_path(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf2ad7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(w_sim, lw=4, linestyle='--', label='learned policy')\n",
    "ax1.plot(w_opt, lw=2, label='optimal')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Wealth')\n",
    "ax1.set_title('Wealth over time')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(c_sim, lw=4, linestyle='--', label='learned policy')\n",
    "ax2.plot(c_opt, lw=2, label='optimal')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Consumption')\n",
    "ax2.set_title('Consumption over time')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1132be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
