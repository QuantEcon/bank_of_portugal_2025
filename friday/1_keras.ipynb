{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce27e84",
   "metadata": {},
   "source": [
    "# Simple Neural Network Regression with Keras and JAX\n",
    "\n",
    "**Prepared for the Bank of Portugal Computational Economics Course (Oct 2025)**\n",
    "\n",
    "**Author:** [John Stachurski](https://johnstachurski.net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0a4ea",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "In this lecture we show how to implement one-dimensional nonlinear regression\n",
    "using a multilayer perceptron (i.e., neural network).\n",
    "\n",
    "We will use the popular deep learning library [Keras](https://keras.io/), which\n",
    "provides a simple interface to deep learning.\n",
    "\n",
    "The emphasis in Keras is on providing an intuitive API, while the heavy lifting is\n",
    "done by one of several possible backends.\n",
    "\n",
    "Currently the backend library options are Tensorflow, PyTorch, and JAX.\n",
    "\n",
    "In this lecture we will set the backend to JAX.\n",
    "\n",
    "Our main aim is to provide a very simple introduction to deep\n",
    "learning in a regression setting.\n",
    "\n",
    "Later, in [a separate lecture](https://jax.quantecon.org/jax_nn.html), we will investigate how to do the same learning task using pure JAX, rather than relying on Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b8e41",
   "metadata": {},
   "source": [
    "If you run this lecture on [Google Colab](https://colab.research.google.com/), set the runtime environment to include a GPU.\n",
    "\n",
    "To run this lecture on your own machine, you need to install [Google JAX](https://github.com/google/jax).\n",
    "\n",
    "* Obviously, if you install the CPU-only version of JAX, you will not get the benefit of a hardware accelerator and this will affect your timings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654f7a6",
   "metadata": {},
   "source": [
    "If necessary, please install Keras by uncommenting the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048f2ef",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a01cd3",
   "metadata": {},
   "source": [
    "Now we specify that the desired backend is JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b199aa",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'jax'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd17b4c",
   "metadata": {},
   "source": [
    "Now we should be able to import some tools from Keras.\n",
    "\n",
    "(Without setting the backend to JAX, the imports below might fail – unless you have PyTorch or Tensorflow set up.  If you have problems running the next cell in Jupyter, try quitting, running `export KERAS_BACKEND=\"jax\"` and then starting Jupyter on the command line from the same terminal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219b700",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba4fa0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8d772",
   "metadata": {},
   "source": [
    "We'll also use the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fa000",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8149f6",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First let’s write a function to generate some data.\n",
    "\n",
    "The data has the form\n",
    "\n",
    "$$\n",
    "y_i = f(x_i) + \\epsilon_i,\n",
    "    \\qquad i=1, \\ldots, n,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- the input sequence $ (x_i) $ is an evenly-spaced grid,  \n",
    "- $ f $ is a nonlinear transformation, and  \n",
    "- each $ \\epsilon_i $ is independent white noise.  \n",
    "\n",
    "\n",
    "Here’s the function that creates vectors `x` and `y` according to the rule\n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea09fb",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def generate_data(x_min=0,           # Minimum x value\n",
    "                  x_max=5,           # Max x value\n",
    "                  data_size=400,     # Default size for dataset\n",
    "                  seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    x = np.linspace(x_min, x_max, num=data_size)\n",
    "    # y = f(x) + ϵ with f as below\n",
    "    ϵ = 0.2 * np.random.randn(data_size)\n",
    "    y = x**0.5 + np.sin(x) + ϵ\n",
    "    # Transform to column vectors (Keras expects two dimensions, not flat arrays)\n",
    "    x, y = [np.reshape(z, (data_size, 1)) for z in (x, y)]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8040a",
   "metadata": {},
   "source": [
    "Now we generate some data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed62ec8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x, y = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f4bc6",
   "metadata": {},
   "source": [
    "Here’s a plot of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b8b41",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0bcff0",
   "metadata": {},
   "source": [
    "We’ll also use data from the same process for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f21b3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x_validate, y_validate = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cefc5f",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We supply functions to build two types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c06c9",
   "metadata": {},
   "source": [
    "## Regression model\n",
    "\n",
    "The first implements linear regression.\n",
    "\n",
    "This is achieved by constructing a neural network with just one layer, that maps\n",
    "to a single dimension (since the prediction is real-valued).\n",
    "\n",
    "The object `model` will be an instance of `keras.Sequential`, which is used to\n",
    "group a stack of layers into a single prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb2d20",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def build_regression_model():\n",
    "    # Generate an instance of Sequential, to store layers and training attributes\n",
    "    model = Sequential()\n",
    "    # Add a single layer with scalar output\n",
    "    model.add(Dense(units=1))  \n",
    "    # Configure the model for training\n",
    "    model.compile(optimizer=keras.optimizers.SGD(), \n",
    "                  loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a4563",
   "metadata": {},
   "source": [
    "In the function above you can see that\n",
    "\n",
    "- we use stochastic gradient descent to train the model, and  \n",
    "- the loss is mean squared error (MSE).  \n",
    "\n",
    "\n",
    "The call `model.add` adds a single layer the activation function equal to the identity map.\n",
    "\n",
    "MSE is the standard loss function for ordinary least squares regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b02fa6",
   "metadata": {},
   "source": [
    "### Deep Network\n",
    "\n",
    "The next function creates a dense (i.e., fully connected) neural network with\n",
    "3 hidden layers, where each hidden layer maps to a k-dimensional output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753adb4f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def build_nn_model(output_dim=10, num_layers=3, activation_function='tanh'):\n",
    "    # Create a Keras Model instance using Sequential()\n",
    "    model = Sequential()\n",
    "    # Add layers to the network sequentially, from inputs towards outputs\n",
    "    for i in range(num_layers):\n",
    "        model.add(Dense(units=output_dim, activation=activation_function))\n",
    "    # Add a final layer that maps to a scalar value, for regression.\n",
    "    model.add(Dense(units=1))\n",
    "    # Embed training configurations\n",
    "    model.compile(optimizer=keras.optimizers.SGD(), \n",
    "                  loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656874d0",
   "metadata": {},
   "source": [
    "### Tracking errors\n",
    "\n",
    "The following function will be used to plot the MSE of the model during the\n",
    "training process.\n",
    "\n",
    "Initially the MSE will be relatively high, but it should fall at each iteration,\n",
    "as the parameters are adjusted to better fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41834997",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_loss_history(training_history, ax):\n",
    "    \"\"\"\n",
    "    Plot the MSE of the training data as a function of the epochs.  \n",
    "    Each epoch corresponds to one pass through the data set and update of\n",
    "    the parameters.\n",
    "\n",
    "    This function acts on the training history returned by a call to the\n",
    "    `fit` method of a given model.\n",
    "    \"\"\"\n",
    "    epochs = training_history.epoch\n",
    "    training_losses = training_history.history['loss']\n",
    "    validation_losses = training_history.history['val_loss']\n",
    "    ax.plot(epochs, \n",
    "            training_losses, \n",
    "            label='training loss')\n",
    "    # Plot MSE of validation data against epoch\n",
    "    ax.plot(epochs, \n",
    "            validation_losses,\n",
    "            label='validation loss')\n",
    "    # Add labels\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss (Mean squared error)')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2831226",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now let’s go ahead and train our  models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae1f9a",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "We’ll start with linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41bf1b5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "regression_model = build_regression_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877c846",
   "metadata": {},
   "source": [
    "Now we train the model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a6088",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "training_history = regression_model.fit(\n",
    "    x, y, \n",
    "    batch_size=x.shape[0], \n",
    "    verbose=0,\n",
    "    epochs=2000, \n",
    "    validation_data=(x_validate, y_validate)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63743f56",
   "metadata": {},
   "source": [
    "Let’s have a look at the evolution of MSE as the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc01cc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_loss_history(training_history, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322420c",
   "metadata": {},
   "source": [
    "Let’s print the final MSE on the cross-validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ba92f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print(\"Testing loss on the validation set.\")\n",
    "regression_model.evaluate(x_validate, y_validate, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e643841",
   "metadata": {},
   "source": [
    "Here’s our output predictions on the cross-validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d5a70",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "y_predict = regression_model.predict(x_validate, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2bb43",
   "metadata": {},
   "source": [
    "We use the following function to plot our predictions along with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5c0d0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_results(x, y, y_predict, ax):\n",
    "    ax.scatter(x, y)\n",
    "    ax.plot(x, y_predict, label=\"fitted model\", color='black')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c933e9",
   "metadata": {},
   "source": [
    "Let’s now call the function on the cross-validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866edd84",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_results(x_validate, y_validate, y_predict, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e89f81",
   "metadata": {},
   "source": [
    "### Deep learning\n",
    "\n",
    "Now let’s switch to a neural network with multiple layers.\n",
    "\n",
    "We implement the same steps as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de42a4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "nn_model = build_nn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c88e38",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "training_history = nn_model.fit(\n",
    "    x, y, \n",
    "    batch_size=x.shape[0], \n",
    "    verbose=0,\n",
    "    epochs=2000, \n",
    "    validation_data=(x_validate, y_validate)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ba7ea",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_loss_history(training_history, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78430b1f",
   "metadata": {},
   "source": [
    "Here’s the final MSE for the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb36c58",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print(\"Testing loss on the validation set.\")\n",
    "nn_model.evaluate(x_validate, y_validate, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d199e",
   "metadata": {},
   "source": [
    "You will notice that this loss is much lower than the one we achieved with\n",
    "linear regression, suggesting a better fit.\n",
    "\n",
    "To confirm this, let’s look at the fitted function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4516082",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "y_predict = nn_model.predict(x_validate, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b6a71",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_results(x, y, y_predict, ax):\n",
    "    ax.scatter(x, y)\n",
    "    ax.plot(x, y_predict, label=\"fitted model\", color='black')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b78a0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_results(x_validate, y_validate, y_predict, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109afd07",
   "metadata": {},
   "source": [
    "Not surprisingly, the multilayer neural network does a much better job of fitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395dbc32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
